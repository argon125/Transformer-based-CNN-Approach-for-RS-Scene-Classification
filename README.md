# Transformer-based-CNN-Approach-for-RS-Scene-Classification
Submission to EAAI Journal


Feature extraction in remote sensing is a challenging yet crucial operation for scene
classification because of cloud cover and overlapping edges present in the data. We present an
analysis of different deep learning architectures on multiple scene classification datasets, to
understand the features and weigh the advantages of one or more functional blocks (such as
convolution and attention mechanism) connected in different convolutional neural networks.
The work takes into consideration five open-source benchmark datasets: UC-Merced Land
Use, WHU-RS19, Optimal-31, RSI-CB256, and MLRSNet, that have been openly made
available to the research community. To perform this task, architectures such as VGG-16,
Resnet50, EfficientNetB3, Vision Transformers (ViT), Swin Transformers, and ConvNeXt is
used. They are fine-tuned using LinBnDrop block from the fastai framework. Though the
comparison between deep learning models for scene classification has been done, the
comparison between different Transformer-based architectures along with the convolutionbased architectures has not been systematically addressed in the remote sensing literature. We
have obtained a new benchmark, that exceeds the state-of-the-art results for all the datasets on
a 90:10 train-test split.

Keywords: Convolutional Neural Networks; Remote Sensing; Scene Classification; fastai

Paper available: Journal under review

Codes: Will be made available shortly
